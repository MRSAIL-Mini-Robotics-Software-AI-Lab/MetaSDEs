# -*- coding: utf-8 -*-
"""MAML Adaptive Implementation.ipynb

Automatically generated by Colaboratory.
"""

# from google.colab import drive
# drive.mount('/content/drive')
# %cd drive/MyDrive/Projects/MAML
# !ls

# Commented out IPython magic to ensure Python compatibility.
# %ls

from torch.optim import Adam, SGD
from copy import deepcopy
from torch import autograd
from tqdm import tqdm
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch import autograd  # pylint: disable=unused-import
from torch.utils import tensorboard
# import util  # pylint: disable=unused-import
import util
import omniglot

from models import simpleNet

BATCH_SIZE = 16
NUM_WAY = 5
NUM_SUPPORT = 1
NUM_QUERY = 19
NUM_TRAINING_ITERATIONS = 15000
NUM_TRAINING_TASKS = BATCH_SIZE*(NUM_TRAINING_ITERATIONS)
NUM_TEST_TASKS = 600

trainloader = omniglot.get_omniglot_dataloader(
    'train',
    BATCH_SIZE,
    NUM_WAY,
    NUM_SUPPORT,
    NUM_QUERY,
    NUM_TRAINING_TASKS
)

validloader = omniglot.get_omniglot_dataloader(
    'val',
    BATCH_SIZE,
    NUM_WAY,
    NUM_SUPPORT,
    NUM_QUERY,
    BATCH_SIZE * 4
)

dataloader_test = omniglot.get_omniglot_dataloader(
    'test',
    1,
    NUM_WAY,
    NUM_SUPPORT,
    NUM_QUERY,
    NUM_TEST_TASKS
)


class MAML:
    def __init__(self, inner_steps: int,
                 inner_lr: float,
                 outer_lr: float,
                 model_class: type,
                 *model_args: any):
        '''
        initiates the MAML class

        Parameters:
        -----------
        inner_steps: int - number of gradient descent update steps
        inner_lr: float - learning rate of the inner gradient step with the support dset
        outer_lr: float - learning rate of the outer gradient step with the query dset
        model_class: type - the model used for metalearning
        '''
        self.inner_steps = inner_steps
        self.inner_lr = nn.Parameter(torch.Tensor([inner_lr]))
        self.outer_lr = nn.Parameter(torch.Tensor([outer_lr]))
        self.model_class = model_class

    def __cloneModule(self, target: nn.Module, instance: nn.Module):
        '''
        clones the input module in a way to be present in the computational graph

        Parameters:
        -----------
        target: torch.nn - the target model to clone
        instance: torch.nn - instance of the same class to clone into
        '''
        # clone parameters of the current level
        for param_key in target._parameters:
            if target._parameters[param_key] is not None:
                instance._parameters[param_key] = target._parameters[param_key].clone(
                )

        # recursively go to the other children parameters and do the same
        for target_child, instance_child in zip(target.children(), instance.children()):
            self.__cloneModule(target_child, instance_child)

    def __innerLoop(self, model: nn.Module, images: torch.Tensor, labels: torch.Tensor = None) -> tuple:
        '''
        performs a GD step update on a given model using the given loss fn

        Parameters:
        -----------
        model: nn.Module - cloned model
        labels: torch.Tensor - loss function
        '''
        # perform gradient updates
        model.train()
        device = labels.device

        for _ in range(self.inner_steps):
            if labels is not None:
                loss = model.loss(images, labels)
            else:
                loss = model.loss(images)

            grads = autograd.grad(
                loss, [*model.parameters()], allow_unused=True, create_graph=True)
            for param, grad in zip(model.parameters(), grads):
                param.data -= self.inner_lr.to(device)*grad

        model.eval()
        with torch.no_grad():
            outs = model.forward(images)
            accuracy_inner = None

            if labels is not None:
                loss = model.loss(images, labels)
                accuracy_inner = util.score(outs, labels)
            else:
                loss = model.loss(images)

        # @NOTE reset the finetune model to the training mode (for cautions)
        model.train()

        return model, loss.item(), accuracy_inner

    def train(self,
              trainloader,
              validloader,
              print_every=25,
              print_valid_every=50,
              pth_filepath='./finetuned_model.pth'
              ):
        '''
        trains the model using MAML finetuning

        Parameters:
        -----------
        trainloader: list - contains Tensors of task batches
        validloader: list - contains Tensors of task batches 
        criterion: callable - loss function
        epochs: int - number of training iterations

        Returns:
        --------
        sli_arr: arr - support loss mean for each epoch for the inner loop
        slo_arr: arr - support loss mean for each epoch for the outerloop
        qli_arr: arr - query loss mean for each epoch for the outerloop
        '''
        # presets
        fineTuneModel = self.model_class()
        targetModel = self.model_class()
        optimizer = SGD([*targetModel.parameters()], lr=self.outer_lr.item())

        # pre adaptation support batch loss and accuracy memo
        support_batch_acc_memo = []
        support_batch_loss_memo = []
        # pre adaptation query batch loss and accuracy memo
        pre_adapt_query_batch_acc_memo = []
        pre_adapt_query_batch_loss_memo = []
        # post adaptation query batch loss and accuracy memo
        post_adapt_query_batch_acc_memo = []
        post_adapt_query_batch_loss_memo = []

        # put model on active device
        device = 'cuda' if torch.cuda.is_available else 'cpu'
        print("[Message] Training on", device)
        fineTuneModel = fineTuneModel.to(device)
        targetModel = targetModel.to(device)

        # model parameters tracking
        best_query_valid_loss = np.Inf

        for i, task_batch in tqdm(enumerate(trainloader)):

            # pre adaptation support batch losses and accs (for each batch)
            support_batch_acc = []
            support_batch_loss = []
            # pre adaptation query batch losses and accs (for each batch)
            pre_adapt_query_batch_acc = []
            pre_adapt_query_batch_loss = []
            # post adaptation query batch losses and accs (for each batch)
            post_adaptation_query_batch_loss = []
            post_adaptation_query_batch_acc = []

            for images_s,  labels_s, images_q, labels_q in task_batch:

                # clone models
                self.__cloneModule(targetModel, fineTuneModel)

                # putting data in device and setting training mode
                targetModel.train()
                fineTuneModel.train()

                images_s, labels_s = images_s.to(device), labels_s.to(device)
                images_q, labels_q = images_q.to(device), labels_q.to(device)

                _, inner_support_loss, inner_support_accuracies = self.__innerLoop(
                    fineTuneModel, images_s, labels_s)

                # outerloop and update
                outs = fineTuneModel(images_q)
                loss = fineTuneModel.loss(images_q, labels_q)

                # calculate pre-adaptation scores
                support_batch_acc.append(inner_support_accuracies)
                support_batch_loss.append(inner_support_loss)
                pre_adapt_query_batch_acc.append(util.score(outs, labels_q))
                pre_adapt_query_batch_loss.append(loss.item())

                # update
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # calculate post-adaptation scores
                targetModel.eval()

                self.__cloneModule(targetModel, fineTuneModel)

                self.__innerLoop(fineTuneModel,
                                 images_q, labels_q)
                outs = fineTuneModel(images_q)
                loss = fineTuneModel.loss(images_q, labels_q)
                post_adaptation_query_batch_acc.append(
                    util.score(outs, labels_q))
                post_adaptation_query_batch_loss.append(loss.item())

            if (i % print_every) == 0:
                # support batch scores
                support_batch_acc = np.mean(
                    support_batch_acc)
                support_batch_loss = np.mean(
                    support_batch_loss)
                support_batch_acc_memo.append(
                    support_batch_acc)
                support_batch_loss_memo.append(
                    support_batch_loss)

                # pre adaptation query batch scores
                pre_adapt_query_batch_acc = np.mean(pre_adapt_query_batch_acc)
                pre_adapt_query_batch_loss = np.mean(
                    pre_adapt_query_batch_loss)
                pre_adapt_query_batch_acc_memo.append(
                    pre_adapt_query_batch_acc)
                pre_adapt_query_batch_loss_memo.append(
                    pre_adapt_query_batch_loss)

                # post adaptations query batch scores
                post_adaptation_query_batch_acc = np.mean(
                    post_adaptation_query_batch_acc)
                post_adaptation_query_batch_loss = np.mean(
                    post_adaptation_query_batch_loss)
                post_adapt_query_batch_loss_memo.append(
                    post_adaptation_query_batch_acc)
                post_adapt_query_batch_acc_memo.append(
                    post_adaptation_query_batch_loss)

                # verbose message
                message = f'''
                [+ Training Log @ iter {i}]----------------------------
                [Inner Support Scores]----
                -support accuracy=\t{support_batch_acc}
                -support loss=\t{support_batch_loss}
                [Pre-Adaptation Query Scores]----
                -accuracy =\t{pre_adapt_query_batch_acc}
                -loss =\t{pre_adapt_query_batch_loss}
                ||-----[After Meta training]--->>
                [Post-Adaptation Query Scores]----
                -accuracy =\t{post_adaptation_query_batch_acc}
                -loss =\t{post_adaptation_query_batch_loss}
                '''
                print(message)

            # Model Validation
            # @NOTE clone the target model into the finetune model and insert it into this function to manipulate it
            self.__cloneModule(targetModel, fineTuneModel)
            valid_losses = self.validateModel(
                fineTuneModel, validloader, generative=False)

            if valid_losses['valid_post_adapt_query_loss'] < best_query_valid_loss:
                message = f'''
                ---
                [+Prompt message] Valid query loss decreased {best_query_valid_loss} --> {valid_losses['valid_post_adapt_query_loss']} 
                 so saving new model in {pth_filepath}
                ---
                '''
                torch.save(targetModel.state_dict(), pth_filepath)
                best_query_valid_loss = valid_losses['valid_post_adapt_query_loss']
                print(message)

            if i % print_valid_every == 0:
                message = f'''
                ------------------------------------
                ------------[Validation]------------
                ------------------------------------
                [Inner Support Scores]----
                -support accuracy=\t{valid_losses['valid_support_acc']}
                -support loss=\t{valid_losses['valid_support_loss']}
                ||-----[After Meta training]--->>
                [Post-Adaptation Query Scores]----
                -accuracy =\t{valid_losses['valid_post_adapt_query_acc']}
                -loss =\t{valid_losses['valid_post_adapt_query_loss']}
                '''
                print(message)

        self.targetModel = targetModel
        return targetModel

    def validateModel(self, valid_model, validloader, generative=False):
        '''
        validates input data

        Parameters
        ----------
        model: model object -  meta trained model
        validloader: iter - torch validation loader

        Returns
        ------
        losses: dict - dictionary of losses (averaged on all batches)  {
            'valid_support_acc': ...,
            'valid_support_loss': ...,
            'valid_post_adapt_query_acc': ...,
            'valid_post_adapt_query_loss': ...,
        }
        '''
        # pre adaptation support batch loss and accuracy memo
        valid_support_batch_acc_memo = [] if not generative else None
        valid_support_batch_loss_memo = []
        # post adaptation query batch loss and accuracy memo
        valid_post_adapt_query_batch_acc_memo = [] if not generative else None
        valid_post_adapt_query_batch_loss_memo = []

        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        valid_model = valid_model.to(device)

        for task_batch in validloader:
            # pre adaptation support batch losses and accs (for each batch)
            valid_support_batch_acc = [] if not generative else None
            valid_support_batch_loss = []
            # post adaptation query batch losses and accs (for each batch)
            valid_post_adaptation_query_batch_loss = []
            valid_post_adaptation_query_batch_acc = [] if not generative else None

            # validation
            for images_s, labels_s, images_q, labels_q in task_batch:
                images_s, labels_s = images_s.to(device), labels_s.to(device)
                images_q, labels_q = images_q.to(device), labels_q.to(device)

                _, inner_support_loss, inner_support_accuracies = self.__innerLoop(valid_model,
                                                                                   images_s, labels_s)
                if not generative:
                    valid_support_batch_acc.append(inner_support_accuracies)

                valid_support_batch_loss.append(inner_support_loss)

                loss = valid_model.loss(images_q, labels_q)

                valid_post_adaptation_query_batch_loss.append(loss.item())

                if not generative:
                    outs = valid_model.forward(images_q)
                    valid_post_adaptation_query_batch_acc.append(
                        util.score(outs, labels_q))
            if not generative:
                valid_support_batch_acc_memo.append(
                    np.mean(valid_support_batch_acc))
            else:
                valid_support_batch_acc_memo = [0]

            valid_support_batch_loss_memo.append(np.mean(
                valid_support_batch_loss))

            if not generative:
                valid_post_adapt_query_batch_acc_memo.append(np.mean(
                    valid_post_adaptation_query_batch_acc))
            else:
                valid_post_adapt_query_batch_acc_memo = [0]

            valid_post_adapt_query_batch_loss_memo.append(np.mean(
                valid_post_adaptation_query_batch_loss))

        losses = {
            'valid_support_acc': np.mean(valid_support_batch_acc_memo) or 'Not supported in generative mode',
            'valid_support_loss': np.mean(valid_support_batch_loss_memo),
            'valid_post_adapt_query_acc': np.mean(valid_post_adapt_query_batch_acc_memo) or 'Not supported in generative mode',
            'valid_post_adapt_query_loss': np.mean(valid_post_adapt_query_batch_loss_memo),
        }

        return losses


if __name__ == "__main__":
    model = MAML(2, 1, 1, lambda: simpleNet(5))
    # model = MAML(2, 1e-3, 1, lambda: simpleNet(5))
    model.train(trainloader=trainloader,
                validloader=validloader, print_every=50)
    torch.save(model.targetModel.state_dict(), 'targetModel.pth')
